{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Song Classification, Part 2\n",
    "\n",
    "Welcome to Lab 5! We'll pick off where we left off and continue to build a song classifer using k-nearest neighbors. \n",
    "\n",
    "Please complete lab 4 before starting lab 5.\n",
    "\n",
    "Lab 4 is part 1 of the investigation. Lab 5 is part 2 of the investigation.\n",
    "\n",
    "You will build a classifier that guesses whether a song is hip-hop or country, using only the number of times that each word appears in a song's lyrics.  By the end of the project, you should know how to:\n",
    "\n",
    "1. Clean and organize a dataset used to test a machine learning model\n",
    "2. Build a k-nearest neighbors classifier\n",
    "3. Test a classifier on data\n",
    "\n",
    "**Advice.** Develop your answers incrementally. To perform a complicated table manipulation, break it up into steps, perform each step on a different line, give a new name to each result, and check that each intermediate result is what you expect. You can add any additional names or functions you want to the provided cells. \n",
    "\n",
    "To get started, load `datascience`, `numpy`, `plots`, and `gofer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up the notebook, but please don't change it.\n",
    "import numpy as np\n",
    "import math\n",
    "from datascience import *\n",
    "\n",
    "# These lines set up the plotting functionality and formatting.\n",
    "import matplotlib\n",
    "matplotlib.use('Agg', warn=False)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "\n",
    "# These lines load the tests.\n",
    "#from gofer.ok import check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: Recap\n",
    "\n",
    "In lab 4, we completed the following tasks:\n",
    "1. In section 1, we explored the dataset and split the dataset into training data and test data.\n",
    "2. In section 2, we ran through a guided example of the k-Nearest Neightbors (k-NN) classification algorithm.\n",
    "\n",
    "**If you do not remember lab 4, we highly recommend you go back and review it now. It will help you for this lab. **\n",
    "\n",
    "In lab 5, we are going to complete the following tasks:\n",
    "1. Identify some features.\n",
    "2. Define a classifier function using your features and the training set.\n",
    "3. Evaluate its performance (the proportion of correct classifications) on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to set up the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = Table.read_table('lyrics.csv')\n",
    "\n",
    "training_proportion = 11/16\n",
    "\n",
    "num_songs = lyrics.num_rows\n",
    "num_train = int(num_songs * training_proportion)\n",
    "num_valid = num_songs - num_train\n",
    "\n",
    "train_lyrics = lyrics.take(np.arange(num_train))\n",
    "test_lyrics = lyrics.take(np.arange(num_train, num_songs))\n",
    "\n",
    "def most_common(label, table):\n",
    "    return table.group(label).sort('count', descending=True).column(label).item(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to extend our classifier from lab 4 to consider more than two features at a time.\n",
    "\n",
    "Euclidean distance still makes sense with more than two features. For `n` different features, we compute the difference between corresponding feature values for two songs, square each of the `n`  differences, sum up the resulting numbers, and take the square root of the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 1.1 ** <br/>\n",
    "Write a function to compute the Euclidean distance between two **arrays** of features of *arbitrary* (but equal) length.  Use it to compute the distance between the first song in the training set and the first song in the test set, *using all of the features*.  (Remember that the title, artist, and genre of the songs are not features.)\n",
    "\n",
    "**Note:** To convert row objects to arrays, use `np.array`. For example, if `t` was a table, `np.array(t.row(0))` converts row 0 of `t` into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(features1, features2):\n",
    "    \"\"\"The Euclidean distance between two arrays of feature values.\"\"\"\n",
    "    return np.sqrt(sum((features1 - features2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(table,row):\n",
    "    \"\"\"Get the features (everything but title, artist, genre) for a given row in a table as an array\"\"\"\n",
    "    return np.array(table.drop(\"Title\",\"Artist\",\"Genre\").row(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14822770081404466"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_first_to_first = distance(get_features(train_lyrics,0),get_features(test_lyrics,0))\n",
    "distance_first_to_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Creating your own feature set\n",
    "\n",
    "Unfortunately, using all of the features has some downsides.  One clear downside is *computational* -- computing Euclidean distances just takes a long time when we have lots of features.  You might have noticed that in the last question!\n",
    "\n",
    "So we're going to select just 20.  We'd like to choose features that are very *discriminative*. That is, features which lead us to correctly classify as much of the test set as possible.  This process of choosing features that will make a classifier work well is sometimes called *feature selection*, or more broadly *feature engineering*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 1.1.1 ** <br/>\n",
    "Look through the list of features (the labels of the `lyrics` table after the first three).  Choose 20 common words that you think might let you distinguish between country and hip-hop songs. Make sure to choose words that are frequent enough that every song contains at least one of them. Don't just choose the 20 most frequent, though... you can do much better.\n",
    "\n",
    "You might want to come back to this question later to improve your list, once you've seen how to evaluate your classifier.  The first time you answer this question, spend some time looking through the features, but not more than 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Title',\n",
       " 'Artist',\n",
       " 'Genre',\n",
       " 'i',\n",
       " 'the',\n",
       " 'you',\n",
       " 'to',\n",
       " 'and',\n",
       " 'a',\n",
       " 'me',\n",
       " 'it',\n",
       " 'not',\n",
       " 'in',\n",
       " 'my',\n",
       " 'is',\n",
       " 'of',\n",
       " 'your',\n",
       " 'that',\n",
       " 'do',\n",
       " 'on',\n",
       " 'are',\n",
       " 'we',\n",
       " 'am',\n",
       " 'will',\n",
       " 'all',\n",
       " 'for',\n",
       " 'no',\n",
       " 'be',\n",
       " 'have',\n",
       " 'love',\n",
       " 'so',\n",
       " 'know',\n",
       " 'this',\n",
       " 'but',\n",
       " 'with',\n",
       " 'what',\n",
       " 'just',\n",
       " 'when',\n",
       " 'like',\n",
       " 'now',\n",
       " 'que',\n",
       " 'time',\n",
       " 'can',\n",
       " 'come',\n",
       " 'de',\n",
       " 'there',\n",
       " 'go',\n",
       " 'up',\n",
       " 'oh',\n",
       " 'la',\n",
       " 'one',\n",
       " 'they',\n",
       " 'out',\n",
       " 'down',\n",
       " 'get',\n",
       " 'she',\n",
       " 'was',\n",
       " 'see',\n",
       " 'if',\n",
       " 'got',\n",
       " 'never',\n",
       " 'from',\n",
       " 'he',\n",
       " 'feel',\n",
       " 'want',\n",
       " 'let',\n",
       " 'make',\n",
       " 'way',\n",
       " 'say',\n",
       " 'take',\n",
       " 'would',\n",
       " 'as',\n",
       " 'ca',\n",
       " 'day',\n",
       " 'at',\n",
       " 'babi',\n",
       " 'away',\n",
       " 'life',\n",
       " 'yeah',\n",
       " 'y',\n",
       " 'back',\n",
       " 'by',\n",
       " 'her',\n",
       " 'heart',\n",
       " 'here',\n",
       " 'how',\n",
       " 'could',\n",
       " 'night',\n",
       " 'need',\n",
       " 'our',\n",
       " 'look',\n",
       " 'where',\n",
       " 'en',\n",
       " 'eye',\n",
       " 'thing',\n",
       " 'world',\n",
       " 'more',\n",
       " 'caus',\n",
       " 'gonna',\n",
       " 'die',\n",
       " 'right',\n",
       " 'been',\n",
       " 'tell',\n",
       " 'think',\n",
       " 'un',\n",
       " 'who',\n",
       " 'el',\n",
       " 'through',\n",
       " 'man',\n",
       " 'live',\n",
       " 'again',\n",
       " 'give',\n",
       " 'too',\n",
       " 'onli',\n",
       " 'te',\n",
       " 'tri',\n",
       " 'tu',\n",
       " 'or',\n",
       " 'whi',\n",
       " 'se',\n",
       " 'keep',\n",
       " 'dream',\n",
       " 'well',\n",
       " 'mind',\n",
       " 'an',\n",
       " 'wo',\n",
       " 'still',\n",
       " 'us',\n",
       " 'his',\n",
       " 'long',\n",
       " 'mi',\n",
       " 'girl',\n",
       " 'wanna',\n",
       " 'find',\n",
       " 'fall',\n",
       " 'around',\n",
       " 'good',\n",
       " 'about',\n",
       " 'some',\n",
       " 'over',\n",
       " 'littl',\n",
       " 'turn',\n",
       " 'hand',\n",
       " 'then',\n",
       " 'noth',\n",
       " 'light',\n",
       " 'said',\n",
       " 'call',\n",
       " 'everi',\n",
       " 'alway',\n",
       " 'were',\n",
       " 'did',\n",
       " 'into',\n",
       " 'e',\n",
       " 'wait',\n",
       " 'leav',\n",
       " 'home',\n",
       " 'had',\n",
       " 'hold',\n",
       " 'run',\n",
       " 'es',\n",
       " 'walk',\n",
       " 'ever',\n",
       " 'them',\n",
       " 'lie',\n",
       " 'face',\n",
       " 'end',\n",
       " 'hear',\n",
       " 'gone',\n",
       " 'head',\n",
       " 'believ',\n",
       " 'everyth',\n",
       " 'ya',\n",
       " 'has',\n",
       " 'cri',\n",
       " 'alon',\n",
       " 'yo',\n",
       " 'hey',\n",
       " 'their',\n",
       " 'le',\n",
       " 'someth',\n",
       " 'si',\n",
       " 'insid',\n",
       " 'du',\n",
       " 'befor',\n",
       " 'o',\n",
       " 'word',\n",
       " 'stay',\n",
       " 'place',\n",
       " 'much',\n",
       " 'chang',\n",
       " 'ich',\n",
       " 'god',\n",
       " 'stand',\n",
       " 'these',\n",
       " 'better',\n",
       " 'last',\n",
       " 'friend',\n",
       " 'soul',\n",
       " 'anoth',\n",
       " 'je',\n",
       " 'left',\n",
       " 'new',\n",
       " 'him',\n",
       " 'than',\n",
       " 'stop',\n",
       " 'tonight',\n",
       " 'burn',\n",
       " 'sun',\n",
       " 'off',\n",
       " 'por',\n",
       " 'lo',\n",
       " 'play',\n",
       " 'thought',\n",
       " 'sing',\n",
       " 'show',\n",
       " 'lost',\n",
       " 'peopl',\n",
       " 'made',\n",
       " 'break',\n",
       " 'und',\n",
       " 'start',\n",
       " 'should',\n",
       " 'realli',\n",
       " 'chorus',\n",
       " 'hard',\n",
       " 'sky',\n",
       " 'old',\n",
       " 'da',\n",
       " 'even',\n",
       " 'boy',\n",
       " 'pleas',\n",
       " 'song',\n",
       " 'care',\n",
       " 'without',\n",
       " 'hope',\n",
       " 'watch',\n",
       " 'pain',\n",
       " 'amor',\n",
       " 'move',\n",
       " 'wrong',\n",
       " 'put',\n",
       " 'free',\n",
       " 'gotta',\n",
       " 'name',\n",
       " 'et',\n",
       " 'same',\n",
       " 'blue',\n",
       " 'con',\n",
       " 'danc',\n",
       " 'talk',\n",
       " 'seem',\n",
       " 'own',\n",
       " 'mine',\n",
       " 'dark',\n",
       " 'na',\n",
       " 'far',\n",
       " 'tear',\n",
       " 'true',\n",
       " 'fire',\n",
       " 'use',\n",
       " 'dead',\n",
       " 'bring',\n",
       " 'other',\n",
       " 'sin',\n",
       " 'someon',\n",
       " 'myself',\n",
       " 'forev',\n",
       " 'close',\n",
       " 'sleep',\n",
       " 'rain',\n",
       " 'those',\n",
       " 'must',\n",
       " 'una',\n",
       " 'les',\n",
       " 'sweet',\n",
       " 'mean',\n",
       " 'fight',\n",
       " 'rememb',\n",
       " 'star',\n",
       " 'while',\n",
       " 'kiss',\n",
       " 'kill',\n",
       " 'high',\n",
       " 'breath',\n",
       " 'doe',\n",
       " 'yes',\n",
       " 'al',\n",
       " 'wish',\n",
       " 'che',\n",
       " 'real',\n",
       " 'smile',\n",
       " 'two',\n",
       " 'cold',\n",
       " 'lord',\n",
       " 'year',\n",
       " 'open',\n",
       " 'il',\n",
       " 'fear',\n",
       " 'di',\n",
       " 'done',\n",
       " 'side',\n",
       " 'fli',\n",
       " 'black',\n",
       " 'line',\n",
       " 'rock',\n",
       " 'blood',\n",
       " 'lose',\n",
       " 'found',\n",
       " 'onc',\n",
       " 'non',\n",
       " 'door',\n",
       " 'bad',\n",
       " 'ooh',\n",
       " 'wonder',\n",
       " 'ti',\n",
       " 'como',\n",
       " 'enough',\n",
       " 'para',\n",
       " 'sound',\n",
       " 'togeth',\n",
       " 'mayb',\n",
       " 'los',\n",
       " 'help',\n",
       " 'shine',\n",
       " 'mani',\n",
       " 'heaven',\n",
       " 'becaus',\n",
       " 'work',\n",
       " 'touch',\n",
       " 'behind',\n",
       " 'yourself',\n",
       " 'today',\n",
       " 'big',\n",
       " 'might',\n",
       " 'death',\n",
       " 'hate',\n",
       " 'miss',\n",
       " 'ride',\n",
       " 'street',\n",
       " 'came',\n",
       " 'ask',\n",
       " 'hide',\n",
       " 'everybodi',\n",
       " 'save',\n",
       " 'till',\n",
       " 'may',\n",
       " 'beauti',\n",
       " 'roll',\n",
       " 'sure',\n",
       " 'deep',\n",
       " 'arm',\n",
       " 'son',\n",
       " 'wind',\n",
       " 'bodi',\n",
       " 'forget',\n",
       " 'happi',\n",
       " 'understand',\n",
       " 'ai',\n",
       " 'best',\n",
       " 'ma',\n",
       " 'town',\n",
       " 'war',\n",
       " 'ah',\n",
       " 'listen',\n",
       " 'seen',\n",
       " 'der',\n",
       " 'sometim',\n",
       " 'lone',\n",
       " 'hell',\n",
       " 'morn',\n",
       " 'until',\n",
       " 'set',\n",
       " 'hurt',\n",
       " 'told',\n",
       " 'first',\n",
       " 'knew',\n",
       " 'goe',\n",
       " 'round',\n",
       " 'em',\n",
       " 'alright',\n",
       " 'music',\n",
       " 'sea',\n",
       " 'truth',\n",
       " 'qui',\n",
       " 'beat',\n",
       " 'nicht',\n",
       " 'eu',\n",
       " 'las',\n",
       " 'todo',\n",
       " 'pass',\n",
       " 'under',\n",
       " 'del',\n",
       " 'late',\n",
       " 'after',\n",
       " 'ani',\n",
       " 'ground',\n",
       " 'fool',\n",
       " 'pas',\n",
       " 'broken',\n",
       " 'goodby',\n",
       " 'das',\n",
       " 'des',\n",
       " 'kind',\n",
       " 'wall',\n",
       " 'angel',\n",
       " 'est',\n",
       " 'road',\n",
       " 'reason',\n",
       " 'dan',\n",
       " 'money',\n",
       " 'chanc',\n",
       " 'quiero',\n",
       " 'mai',\n",
       " 'part',\n",
       " 'matter',\n",
       " 'learn',\n",
       " 'els',\n",
       " 'rise',\n",
       " 'pour',\n",
       " 'saw',\n",
       " 'grow',\n",
       " 'ha',\n",
       " 'sit',\n",
       " 'land',\n",
       " 'fade',\n",
       " 'air',\n",
       " 'took',\n",
       " 'nobodi',\n",
       " 'ne',\n",
       " 'fill',\n",
       " 'easi',\n",
       " 'game',\n",
       " 'follow',\n",
       " 'wake',\n",
       " 'anyth',\n",
       " 'aliv',\n",
       " 'solo',\n",
       " 'water',\n",
       " 'heard',\n",
       " 'though',\n",
       " 'white',\n",
       " 'vida',\n",
       " 'ist',\n",
       " 'voic',\n",
       " 'woman',\n",
       " 'blind',\n",
       " 'each',\n",
       " 'citi',\n",
       " 'young',\n",
       " 'lay',\n",
       " 'moon',\n",
       " 'reach',\n",
       " 'readi',\n",
       " 'fine',\n",
       " 'soon',\n",
       " 'along',\n",
       " 'à',\n",
       " 'crazi',\n",
       " 'past',\n",
       " 'red',\n",
       " 'room',\n",
       " 'such',\n",
       " 'ein',\n",
       " 'step',\n",
       " 'scream',\n",
       " 'tomorrow',\n",
       " 'control',\n",
       " 'memori',\n",
       " 'strong',\n",
       " 'hit',\n",
       " 'everyon',\n",
       " 'wast',\n",
       " 'não',\n",
       " 'men',\n",
       " 'earth',\n",
       " 's',\n",
       " 'upon',\n",
       " 'final',\n",
       " 'shake',\n",
       " 'im',\n",
       " 'abov',\n",
       " 'ja',\n",
       " 'moment',\n",
       " 'rest',\n",
       " 'guess',\n",
       " 'hous',\n",
       " 'born',\n",
       " 'somebodi',\n",
       " 'feet',\n",
       " 'mas',\n",
       " 'sad',\n",
       " 'laugh',\n",
       " 'esta',\n",
       " 'happen',\n",
       " 'went',\n",
       " 'cest',\n",
       " 'blow',\n",
       " 'noch',\n",
       " 'meet',\n",
       " 'whole',\n",
       " 'lover',\n",
       " 'drive',\n",
       " 'car',\n",
       " 'lead',\n",
       " 'su',\n",
       " 'veri',\n",
       " 'ni',\n",
       " 'promis',\n",
       " 'bed',\n",
       " 'apart',\n",
       " 'um',\n",
       " 'empti',\n",
       " 'shadow',\n",
       " 'full',\n",
       " 'begin',\n",
       " 'pull',\n",
       " 'child',\n",
       " 'pretti',\n",
       " 'drink',\n",
       " 'pray',\n",
       " 'king',\n",
       " 'cut',\n",
       " 't',\n",
       " 'power',\n",
       " 'gave',\n",
       " 'hay',\n",
       " 'peac',\n",
       " 'hang',\n",
       " 'next',\n",
       " 'ser',\n",
       " 'mother',\n",
       " 'é',\n",
       " 'cuando',\n",
       " 'between',\n",
       " 'pay',\n",
       " 'carri',\n",
       " 'tire',\n",
       " 'speak',\n",
       " 'lot',\n",
       " 'tree',\n",
       " 'den',\n",
       " 'skin',\n",
       " 'tout',\n",
       " 'til',\n",
       " 'slow',\n",
       " 'sinc',\n",
       " 'anymor',\n",
       " 'kid',\n",
       " 'clear',\n",
       " 'mich',\n",
       " 'jesus',\n",
       " 'differ',\n",
       " 'fast',\n",
       " 'becom',\n",
       " 'va',\n",
       " 'gun',\n",
       " 'nos',\n",
       " 'zu',\n",
       " 'más',\n",
       " 'river',\n",
       " 'faith',\n",
       " 'floor',\n",
       " 'cross',\n",
       " 'ladi',\n",
       " 'near',\n",
       " 'fun',\n",
       " 'ring',\n",
       " 'answer',\n",
       " 'pero',\n",
       " 'afraid',\n",
       " 'jag',\n",
       " 'repeat',\n",
       " 'great',\n",
       " 'stori',\n",
       " 'yet',\n",
       " 'cannot',\n",
       " 'hot',\n",
       " 'wir',\n",
       " 'nada',\n",
       " 'hour',\n",
       " 'summer',\n",
       " 'stone',\n",
       " 'bleed',\n",
       " 'sign',\n",
       " 'mir',\n",
       " 'lyric',\n",
       " 'children',\n",
       " 'trust',\n",
       " 'wild',\n",
       " 'felt',\n",
       " 'troubl',\n",
       " 'honey',\n",
       " 'sick',\n",
       " 'perfect',\n",
       " 'human',\n",
       " 'bright',\n",
       " 'bien',\n",
       " 'mama',\n",
       " 'dich',\n",
       " 'parti',\n",
       " 'train',\n",
       " 'plan',\n",
       " 'porqu',\n",
       " 'sorri',\n",
       " 'plus',\n",
       " 'hair',\n",
       " 'secret',\n",
       " 'silenc',\n",
       " 'tast',\n",
       " 'brother',\n",
       " 'throw',\n",
       " 'within',\n",
       " 'per',\n",
       " 'push',\n",
       " 'm',\n",
       " 'somewher',\n",
       " 'three',\n",
       " 'cloud',\n",
       " 'shall',\n",
       " 'vez',\n",
       " 'low',\n",
       " 'window',\n",
       " 'knee',\n",
       " 'lip',\n",
       " 'futur',\n",
       " 'dear',\n",
       " 'search',\n",
       " '2',\n",
       " 'sie',\n",
       " 'drop',\n",
       " 'det',\n",
       " 'cool',\n",
       " 'flame',\n",
       " 'outsid',\n",
       " 'worri',\n",
       " 'bout',\n",
       " 'goin',\n",
       " 'met',\n",
       " 'stare',\n",
       " 'sight',\n",
       " 'most',\n",
       " 'mon',\n",
       " 'voy',\n",
       " 'across',\n",
       " 'wear',\n",
       " 'wave',\n",
       " 'tus',\n",
       " 'pick',\n",
       " 'er',\n",
       " 'send',\n",
       " 'siempr',\n",
       " 'wie',\n",
       " 'bone',\n",
       " 'flow',\n",
       " 'piec',\n",
       " 'soy',\n",
       " 'uh',\n",
       " 'thank',\n",
       " 'mis',\n",
       " 'straight',\n",
       " 'blame',\n",
       " 'catch',\n",
       " 'au',\n",
       " 'ce',\n",
       " 'tight',\n",
       " '&',\n",
       " 'mile',\n",
       " 'holi',\n",
       " 'darl',\n",
       " 'auf',\n",
       " 'mouth',\n",
       " 'christma',\n",
       " 'which',\n",
       " 'moi',\n",
       " 'against',\n",
       " 'mad',\n",
       " 'both',\n",
       " 'share',\n",
       " 'caught',\n",
       " 'father',\n",
       " 'read',\n",
       " 'warm',\n",
       " 'wing',\n",
       " 'nunca',\n",
       " 'belong',\n",
       " 'damn',\n",
       " 'eat',\n",
       " 'win',\n",
       " 'sens',\n",
       " 'comm',\n",
       " 'u',\n",
       " 'fell',\n",
       " 'evil',\n",
       " 'dem',\n",
       " 'ho',\n",
       " 'pictur',\n",
       " 'second',\n",
       " 'nowher',\n",
       " 'dog',\n",
       " 'rule',\n",
       " 'une',\n",
       " 'mal',\n",
       " 'mountain',\n",
       " 'joy',\n",
       " 'mit',\n",
       " 'top',\n",
       " 'tan',\n",
       " 'som',\n",
       " 'bit',\n",
       " 'cover',\n",
       " 'point',\n",
       " 'thousand',\n",
       " 'hoy',\n",
       " 'n',\n",
       " 'gold',\n",
       " 'bell',\n",
       " 'è',\n",
       " 'devil',\n",
       " 'bird',\n",
       " 'sur',\n",
       " 'quit',\n",
       " 'brain',\n",
       " 'return',\n",
       " 'build',\n",
       " 'kick',\n",
       " 'rose',\n",
       " 'babe',\n",
       " 'guy',\n",
       " 'whoa',\n",
       " 'shame',\n",
       " 'mein',\n",
       " 'space',\n",
       " 'whisper',\n",
       " 'tiempo',\n",
       " 'cos',\n",
       " 'write',\n",
       " 'dir',\n",
       " 'tengo',\n",
       " 'count',\n",
       " 'slip',\n",
       " 'ver',\n",
       " 'desir',\n",
       " 'sei',\n",
       " 'nice',\n",
       " 'weak',\n",
       " 'rais',\n",
       " 'jai',\n",
       " 'ocean',\n",
       " 'number',\n",
       " 'drown',\n",
       " 'spirit',\n",
       " 'scare',\n",
       " 'strang',\n",
       " 'ik',\n",
       " 'half',\n",
       " 'known',\n",
       " 'pretend',\n",
       " 'worth',\n",
       " 'someday',\n",
       " 'act',\n",
       " 'beyond',\n",
       " 'fair',\n",
       " 'million',\n",
       " 'finger',\n",
       " 'question',\n",
       " 'doubt',\n",
       " 'meant',\n",
       " 'smoke',\n",
       " 'comin',\n",
       " 'flesh',\n",
       " 'dawn',\n",
       " 'daddi',\n",
       " 'storm',\n",
       " 'hole',\n",
       " 'color',\n",
       " 'jump',\n",
       " 'remain',\n",
       " 'mundo',\n",
       " 'etern',\n",
       " 'nothin',\n",
       " 'buy',\n",
       " 'nur',\n",
       " 'anyon',\n",
       " 'dress',\n",
       " '1',\n",
       " 'minut',\n",
       " 'dont',\n",
       " 'pra',\n",
       " 'snow',\n",
       " 'd',\n",
       " 'dig',\n",
       " 'estoy',\n",
       " 'forc',\n",
       " 'spend',\n",
       " 'shot',\n",
       " 'everyday',\n",
       " 'puedo',\n",
       " 'tú',\n",
       " 'chain',\n",
       " 'dri',\n",
       " 'grace',\n",
       " 'green',\n",
       " 'glori',\n",
       " 'doch',\n",
       " 'everywher',\n",
       " 'freedom',\n",
       " 'realiz',\n",
       " 'lock',\n",
       " 'pride',\n",
       " 'whatev',\n",
       " 'feed',\n",
       " 'mistak',\n",
       " 'vers',\n",
       " 'fate',\n",
       " 'meu',\n",
       " 'escap',\n",
       " 'wash',\n",
       " 'mess',\n",
       " 'você',\n",
       " 'sand',\n",
       " 'front',\n",
       " 'forgiv',\n",
       " 'vi',\n",
       " 'winter',\n",
       " 'tanto',\n",
       " 'magic',\n",
       " 'flower',\n",
       " 'pa',\n",
       " 'closer',\n",
       " 'safe',\n",
       " 'wenn',\n",
       " 'sol',\n",
       " 'bless',\n",
       " 'suffer',\n",
       " 'longer',\n",
       " 'dein',\n",
       " 'hello',\n",
       " 'san',\n",
       " 'grave',\n",
       " 'alreadi',\n",
       " 'yesterday',\n",
       " 'wide',\n",
       " 'radio',\n",
       " 'sorrow',\n",
       " 'ten',\n",
       " 'tien',\n",
       " 'phone',\n",
       " 'crawl',\n",
       " 'surviv',\n",
       " 'ay',\n",
       " 'tie',\n",
       " 'exist',\n",
       " 'anyway',\n",
       " 'io',\n",
       " 'spin',\n",
       " 'insan',\n",
       " 'nous',\n",
       " 'slowli',\n",
       " 'dond',\n",
       " 'ear',\n",
       " 'crowd',\n",
       " 'clean',\n",
       " 'natur',\n",
       " 'knock',\n",
       " 'four',\n",
       " 'poor',\n",
       " 'loud',\n",
       " 'shoe',\n",
       " 'bar',\n",
       " 'está',\n",
       " 'lovin',\n",
       " 'higher',\n",
       " 'race',\n",
       " 'silent',\n",
       " 'lift',\n",
       " 'destroy',\n",
       " 'cuz',\n",
       " 'soft',\n",
       " 'paint',\n",
       " 'mar',\n",
       " 'dirti',\n",
       " 'check',\n",
       " 'ta',\n",
       " 'held',\n",
       " 'vie',\n",
       " 'shout',\n",
       " 'sail',\n",
       " 'twist',\n",
       " 'guitar',\n",
       " 'simpl',\n",
       " 'quand',\n",
       " 'sa',\n",
       " 'ei',\n",
       " 'deni',\n",
       " 'fail',\n",
       " 'van',\n",
       " 'famili',\n",
       " 'vision',\n",
       " 'toi',\n",
       " 'lookin',\n",
       " 'track',\n",
       " 'broke',\n",
       " 'dia',\n",
       " 'state',\n",
       " 'welcom',\n",
       " 'ghost',\n",
       " 'almost',\n",
       " 'shut',\n",
       " 'entr',\n",
       " 'somehow',\n",
       " 'glass',\n",
       " 'yea',\n",
       " 'gent',\n",
       " 'choos',\n",
       " 'für',\n",
       " 'bin',\n",
       " 'glad',\n",
       " 'shoot',\n",
       " 'ell',\n",
       " 'small',\n",
       " 'hill',\n",
       " 'wine',\n",
       " 'treat',\n",
       " 'dos',\n",
       " 'stranger',\n",
       " 'regret',\n",
       " 'refrain',\n",
       " 'corazón',\n",
       " 'machin',\n",
       " 'problem',\n",
       " 'feelin',\n",
       " 'sister',\n",
       " 'era',\n",
       " 'steal',\n",
       " 'endless',\n",
       " 'vai',\n",
       " 'awak',\n",
       " 'key',\n",
       " 'grand',\n",
       " 'cada',\n",
       " 'age',\n",
       " 'bridg',\n",
       " 'dust',\n",
       " 'wit',\n",
       " 'merci',\n",
       " 'wheel',\n",
       " 'ere',\n",
       " 'crack',\n",
       " 'taken',\n",
       " 'prais',\n",
       " 'heal',\n",
       " 'queen',\n",
       " 'mirror',\n",
       " 'von',\n",
       " 'corner',\n",
       " 'ba',\n",
       " 'ago',\n",
       " 'buri',\n",
       " 'book',\n",
       " 'ahora',\n",
       " 'climb',\n",
       " 'quando',\n",
       " 'okay',\n",
       " 'cloth',\n",
       " 'band',\n",
       " 'school',\n",
       " 'stick',\n",
       " 'circl',\n",
       " 'ou',\n",
       " 'tv',\n",
       " 'don',\n",
       " 'strength',\n",
       " 'og',\n",
       " 'five',\n",
       " 'countri',\n",
       " 'beneath',\n",
       " 'sunshin',\n",
       " 'sabe',\n",
       " 'alma',\n",
       " 'ice',\n",
       " 'ça',\n",
       " 'bound',\n",
       " 'less',\n",
       " 'sell',\n",
       " 'tongu',\n",
       " 'path',\n",
       " 'cielo',\n",
       " 'travel',\n",
       " 'hasta',\n",
       " 'hat',\n",
       " 'rather',\n",
       " 'heat',\n",
       " 'innoc',\n",
       " 'confus',\n",
       " 'cosa',\n",
       " 'golden',\n",
       " 'ohh',\n",
       " 'mari',\n",
       " 'singl',\n",
       " ...)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics.labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to get rid of lyrics that don't meet the test criteria (below) of appearing at least 1 time in every song\n",
    "\n",
    "# turns out this is a bad idea \n",
    "# - first of all that wasn't the criteria (at least 1 lyric out of the 20 picked needs to be there\n",
    "# - second it runs forver\n",
    "# - third and most importantly no lyrics meet this criteria!!!\n",
    "\n",
    "#common_lyrics = lyrics\n",
    "#for lyric in lyrics.drop(\"Title\",\"Artist\",\"Genre\").labels:\n",
    "#    if common_lyrics.sort(lyric).column(lyric)[0] == 0:\n",
    "#        common_lyrics = common_lyrics.drop(lyric)\n",
    "#common_lyrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set my_20_features to an array of 20 features (strings that are column labels)\n",
    "\n",
    "my_20_features = \"kiss\",\"kill\",\"lord\",\"blood\",\"love\",\"like\",\"heart\",\"night\",\"die\",\"live\",\"lie\",\"god\",\"soul\",\"friend\",\"fear\",\"lose\",\"hurt\",\"money\",\"car\",\"gun\"\n",
    "\n",
    "train_20 = train_lyrics.select(my_20_features)\n",
    "test_20 = test_lyrics.select(my_20_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's classify the first song from our test set using these features.  You can examine the song by running the cells below. Do you think it will be classified correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Title</th> <th>Artist</th> <th>Genre</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>That Kind of Love</td> <td>Alison Krauss</td> <td>Country</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>kiss</th> <th>kill</th> <th>lord</th> <th>blood</th> <th>love</th> <th>like</th> <th>heart</th> <th>night</th> <th>die</th> <th>live</th> <th>lie</th> <th>god</th> <th>soul</th> <th>friend</th> <th>fear</th> <th>lose</th> <th>hurt</th> <th>money</th> <th>car</th> <th>gun</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0   </td> <td>0   </td> <td>0   </td> <td>0    </td> <td>0.026455</td> <td>0.010582</td> <td>0.015873</td> <td>0    </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0.010582</td> <td>0     </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0    </td> <td>0   </td> <td>0   </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Song:\")\n",
    "test_lyrics.take(0).select('Title', 'Artist', 'Genre').show()\n",
    "print(\"Features:\")\n",
    "test_20.take(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we want to look for the songs in the training set that are most alike our test song.  We will calculate the Euclidean distances from the test song (using the 20 selected features) to all songs in the training set.  You could do this with a `for` loop, but to make it computationally faster, we have provided a function, `fast_distances`, to do this for you.  Read its documentation to make sure you understand what it does.  (You don't need to read the code in its body unless you want to.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell to define fast_distances.\n",
    "\n",
    "def fast_distances(test_row, train_rows):\n",
    "    \"\"\"An array of the distances between test_row and each row in train_rows.\n",
    "\n",
    "    Takes 2 arguments:\n",
    "      test_row: A row of a table containing features of one\n",
    "        test song (e.g., test_20.row(0)).\n",
    "      train_rows: A table of features (for example, the whole\n",
    "        table train_20).\"\"\"\n",
    "    assert train_rows.num_columns < 50, \"Make sure you're not using all the features of the lyrics table.\"\n",
    "    counts_matrix = np.asmatrix(train_rows.columns).transpose()\n",
    "    diff = np.tile(np.array(test_row), [counts_matrix.shape[0], 1]) - counts_matrix\n",
    "    distances = np.squeeze(np.asarray(np.sqrt(np.square(diff).sum(1))))\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 1.1.2 ** <br/>\n",
    "Use the `fast_distances` function provided above to compute the distance from the first song in the test set to all the songs in the training set, **using your set of 20 features**.  Make a new table called `genre_and_distances` with one row for each song in the training set and two columns:\n",
    "* The `\"Genre\"` of the training song\n",
    "* The `\"Distance\"` from the first song in the test set \n",
    "\n",
    "Ensure that `genre_and_distances` is **sorted in increasing order by distance to the first test song**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "for_assignment_type": "student"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Genre</th> <th>Distance</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>Country</td> <td>0.0138862</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Hip-hop</td> <td>0.014038 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Hip-hop</td> <td>0.0159516</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Hip-hop</td> <td>0.0160482</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Country</td> <td>0.0167316</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Country</td> <td>0.0174181</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Country</td> <td>0.0174552</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Hip-hop</td> <td>0.0177446</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Country</td> <td>0.0178515</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Country</td> <td>0.0178621</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (1173 rows omitted)</p>"
      ],
      "text/plain": [
       "Genre   | Distance\n",
       "Country | 0.0138862\n",
       "Hip-hop | 0.014038\n",
       "Hip-hop | 0.0159516\n",
       "Hip-hop | 0.0160482\n",
       "Country | 0.0167316\n",
       "Country | 0.0174181\n",
       "Country | 0.0174552\n",
       "Hip-hop | 0.0177446\n",
       "Country | 0.0178515\n",
       "Country | 0.0178621\n",
       "... (1173 rows omitted)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The staff solution took 4 lines of code.\n",
    "dist = fast_distances(test_20.row(0),train_20)\n",
    "genre_and_distances = train_lyrics.select(\"Genre\").with_column(\"Distance\",dist).sort(\"Distance\")\n",
    "genre_and_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 1.1.3 ** <br/>\n",
    "Now compute the 5-nearest neighbors classification of the first song in the test set.  That is, decide on its genre by finding the most common genre among its 5 nearest neighbors, according to the distances you've calculated.  Then check whether your classifier chose the right genre.  (Depending on the features you chose, your classifier might not get this song right, and that's okay.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The assigned genre, Hip-hop, was not correct.\n"
     ]
    }
   ],
   "source": [
    "# Set my_assigned_genre to the most common genre among these.\n",
    "my_assigned_genre = genre_and_distances.take(np.arange(0,5)).group(\"Genre\").sort(\"count\",descending=True).column(\"Genre\")[0]\n",
    "\n",
    "# Set my_assigned_genre_was_correct to True if my_assigned_genre\n",
    "# matches the actual genre of the first song in the test set.\n",
    "my_assigned_genre_was_correct = my_assigned_genre == test_lyrics.column(\"Genre\")[0]\n",
    "\n",
    "print(\"The assigned genre, {}, was{}correct.\".format(my_assigned_genre, \" \" if my_assigned_genre_was_correct else \" not \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. A classifier function\n",
    "\n",
    "Now we can write a single function that encapsulates the whole process of classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 1.2.1 ** <br/>\n",
    "Write a function called `classify`.  It should take the following four arguments:\n",
    "* A row of features for a song to classify (e.g., `test_20.row(0)`).\n",
    "* A table with a column for each feature (e.g., `train_20`).\n",
    "* An array of classes that has as many items as the previous table has rows, and in the same order.\n",
    "* `k`, the number of neighbors to use in classification.\n",
    "\n",
    "It should return the class a `k`-nearest neighbor classifier picks for the given row of features (the string `'Country'` or the string `'Hip-hop'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test_row, train_rows, train_classes, k):\n",
    "    \"\"\"Return the most common class among k nearest neigbors to test_row.\"\"\"\n",
    "    distances = fast_distances(test_row, train_rows)\n",
    "    genre_and_distances = Table().with_columns(\"Genre\",train_classes,\"Distance\",distances).sort(\"Distance\")\n",
    "    return genre_and_distances.take(np.arange(0,k)).group(\"Genre\").sort(\"count\",descending=True).column(\"Genre\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hip-hop'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(test_20.row(0),train_20,train_lyrics.column(\"Genre\"),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 1.2.2 ** <br/>\n",
    "Assign `grandpa_genre` to the genre predicted by your classifier for the song \"Grandpa Got Runned Over By A John Deere\" in the test set, using **9 neighbors** and using your 20 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Country'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The staff solution first defined a row object called grandpa_features.\n",
    "grandpa_features = lyrics.where(\"Title\",\"Grandpa Got Runned Over By A John Deere\").select(my_20_features).row(0)\n",
    "grandpa_genre = classify(grandpa_features,train_20,train_lyrics.column(\"Genre\"),9)\n",
    "grandpa_genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when we evaluate our classifier, it will be useful to have a classification function that is specialized to use a fixed training set and a fixed value of `k`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 1.2.3 ** <br/>\n",
    "Create a classification function that takes as its argument a row containing your 20 features and classifies that row using the 5-nearest neighbors algorithm with `train_20` as its training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hip-hop'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify_one_argument(row):\n",
    "    return classify(row,train_20,train_lyrics.column(\"Genre\"),5)\n",
    "\n",
    "# When you're done, this should produce 'Hip-hop' or 'Country'.\n",
    "classify_one_argument(test_20.row(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Evaluating your classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's easy to use the classifier, let's see how accurate it is on the whole test set.\n",
    "\n",
    "**Question 1.3.1.** Use `classify_one_argument` and `apply` to classify every song in the test set.  Name these guesses `test_guesses`.  **Then**, compute the proportion of correct classifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6449814126394052"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_guesses = test_20.apply(classify_one_argument)\n",
    "proportion_correct = np.sum(test_guesses==test_lyrics.column(\"Genre\"))/len(test_guesses)\n",
    "proportion_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you've gone through one cycle of classifier design.  Let's summarize the steps:\n",
    "1. From available data, select test and training sets.\n",
    "2. Choose an algorithm you're going to use for classification.\n",
    "3. Identify some features.\n",
    "4. Define a classifier function using your features and the training set.\n",
    "5. Evaluate its performance (the proportion of correct classifications) on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You're finished with lab 5 and Data 8.3x! You've created your own song classifer using k-nearest neighbors. \n",
    "\n",
    "**If you want to continue, you can read about classification online and try to build an even more accurate classifier.**\n",
    "\n",
    "In order to successfully submit your assignment, follow these steps...\n",
    "- **IMPORTANT** Before you do anything, **Save and Checkpoint** from the `File` menu. Please do this first before running the cell below,\n",
    "- **run all the tests and verify that they all pass** (the next cell has a shortcut for that), \n",
    "- **Review the notebook one last time, we will be grading the final state of your notebook** If you make any changes, please **Save and Checkpoint** again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For your convenience, you can run this cell to run all the tests at once!\n",
    "import glob\n",
    "from gofer.ok import grade_notebook\n",
    "if not globals().get('__GOFER_GRADER__', False):\n",
    "    display(grade_notebook('lab05.ipynb', sorted(glob.glob('tests/q*.py'))))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
